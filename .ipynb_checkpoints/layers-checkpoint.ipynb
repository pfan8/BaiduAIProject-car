{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from utils.pickle_io import *\n",
    "import utils.config as config\n",
    "import paddle.fluid as fluid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.fluid as fluid\n",
    "import numpy as np\n",
    "\n",
    "class Encoder(fluid.dygraph.Layer):\n",
    "    def __init__(self, \n",
    "                 name_scope, \n",
    "                 enc_units, \n",
    "                 batch_size, \n",
    "                 vocab_size=1e5, \n",
    "                 word_vector=None,\n",
    "                 param_attr=None,\n",
    "                 bias_attr=None,\n",
    "                 is_reverse=False,\n",
    "                 gate_activation='sigmoid',\n",
    "                 candidate_activation='tanh',\n",
    "                 h_0=None,\n",
    "                 origin_mode=False):\n",
    "        '''\n",
    "            Encoder初始化\n",
    "            :param name_scope: 所在命名空间\n",
    "            :param enc_units:GRU单元维度\n",
    "            :param batch_size: Batch数量\n",
    "            :param wordvector: 自定义词向量\n",
    "            \n",
    "        '''\n",
    "        super(Encoder, self).__init__(name_scope)\n",
    "        self.enc_units = int(enc_units)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.word_vector = word_vector\n",
    "        \n",
    "        # Embedding\n",
    "        if word_vector is not None:\n",
    "            w_param_attrs = fluid.ParamAttr(\n",
    "                                name=\"emb_weight\",\n",
    "                                learning_rate=0.5,\n",
    "                                initializer=fluid.initializer.NumpyArrayInitializer(self.word_vector),\n",
    "                                trainable=True)\n",
    "            self._embedding = fluid.dygraph.Embedding(\n",
    "                                name_scope='embedding',\n",
    "                                size=list(self.word_vector.shape),\n",
    "                                param_attr= w_param_attrs,\n",
    "                                is_sparse=False)\n",
    "            # 如果有自定义词向量维度不符合 D*3，需要添加一层FC\n",
    "            if self.word_vector.shape[1] != self.enc_units*3:\n",
    "                self._fc = fluid.dygraph.FC('fc_for_gru', self.enc_units*3)\n",
    "        else:\n",
    "            self._embedding = fluid.dygraph.Embedding(\n",
    "                                name_scope='embedding',\n",
    "                                size=[self.vocab_size, self.enc_units*3],\n",
    "                                param_attr='emb.w',\n",
    "                                is_sparse=False)\n",
    "        \n",
    "        # GRU\n",
    "        self._gru = fluid.dygraph.GRUUnit(\n",
    "            self.full_name(),\n",
    "            size=self.enc_units * 3,\n",
    "            param_attr=param_attr,\n",
    "            bias_attr=bias_attr,\n",
    "            activation=candidate_activation,\n",
    "            gate_activation=gate_activation,\n",
    "            origin_mode=origin_mode)\n",
    "        self.h_0 = h_0 if h_0 is not None else self.initialize_hidden_state()\n",
    "        self.is_reverse = is_reverse\n",
    "                                                \n",
    "    \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        '''\n",
    "        调用Encoder时的计算\n",
    "        :param inputs: variable类型的输入数据，维度（ N, T, D ）\n",
    "        :param hidden: 隐藏层输入h_0\n",
    "        :return output,state: output = hidden拼接向量，维度（ N, T, H ）\n",
    "                              state = hidden时间维度的最后一个向量\n",
    "        '''\n",
    "        hidden = self.h_0 if hidden is None else hidden\n",
    "        res = []\n",
    "        for i in range(inputs.shape[1]):\n",
    "            if self.is_reverse:\n",
    "                i = inputs.shape[1] - 1 - i\n",
    "            input_ = inputs[:, i:i + 1, :]\n",
    "            input_ = fluid.layers.reshape(\n",
    "                input_, [-1, input_.shape[2]], inplace=False)\n",
    "            input_ = self._embedding(inputs)\n",
    "            if hasattr(self, '_fc'):\n",
    "                input_ = self._fc(input_)\n",
    "            hidden, reset, gate = self._gru(input_, hidden)\n",
    "            hidden_ = fluid.layers.reshape(\n",
    "                hidden, [-1, 1, hidden.shape[1]], inplace=False)\n",
    "            res.append(hidden_)\n",
    "        if self.is_reverse:\n",
    "            res = res[::-1]\n",
    "        res = fluid.layers.concat(res, axis=1)\n",
    "        return res, res[:,-1,:]\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return fluid.layers.zeros((self.batch_size, self.enc_units), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(fluid.dygraph.Layer):\n",
    "    def __init__(self, name_scope, units):\n",
    "        super(BahdanauAttention, self).__init__(name_scope)\n",
    "        self.W1 = fluid.dygraph.FC('attention_fc1', units, num_flatten_dims=2)\n",
    "        self.W2 = fluid.dygraph.FC('attention_fc2', units, num_flatten_dims=2)\n",
    "        self.V = fluid.dygraph.FC('attention_v', 1, num_flatten_dims=2)\n",
    "\n",
    "    def forward(self, query, values):\n",
    "        # query shape == (batch_size, 1, hidden_size)\n",
    "        # values shape == (batch_size, max_length, hidden_size)\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        temp = (self.W1(values) + self.W2(query))\n",
    "        score = self.V(fluid.layers.tanh(temp))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = fluid.layers.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # paddlepaddle will not do broadcast autom atically, use elementwise_mul API\n",
    "        context_vector = fluid.layers.elementwise_mul(values, attention_weights)\n",
    "        context_vector = fluid.layers.reduce_sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(fluid.dygraph.Layer):\n",
    "    def __init__(self, \n",
    "                 name_space, \n",
    "                 dec_units,\n",
    "                 batch_size,\n",
    "                 vocab_size=1e5, \n",
    "                 word_vector=None,\n",
    "                 param_attr=None,\n",
    "                 bias_attr=None,\n",
    "                 is_reverse=False,\n",
    "                 gate_activation='sigmoid',\n",
    "                 candidate_activation='tanh',\n",
    "                 h_0=None,\n",
    "                 origin_mode=False):\n",
    "        super(Decoder, self).__init__(name_space)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.dec_units = int(dec_units)\n",
    "        self.vocab_size = int(vocab_size)\n",
    "        self.word_vector = word_vector\n",
    "        self.attention_units = attention_units\n",
    "        \n",
    "       # Embedding\n",
    "        if word_vector is not None:\n",
    "            w_param_attrs = fluid.ParamAttr(\n",
    "                                name=\"emb_weight\",\n",
    "                                learning_rate=0.5,\n",
    "                                initializer=fluid.initializer.NumpyArrayInitializer(self.word_vector),\n",
    "                                trainable=True)\n",
    "            self._embedding = fluid.dygraph.Embedding(\n",
    "                                name_scope='embedding',\n",
    "                                size=list(self.word_vector.shape),\n",
    "                                param_attr= w_param_attrs,\n",
    "                                is_sparse=False)\n",
    "            self.vocab_size = self._embedding.shape[0]\n",
    "        else:\n",
    "            self._embedding = fluid.dygraph.Embedding(\n",
    "                                name_scope='embedding',\n",
    "                                size=[self.vocab_size, self.dec_units*3],\n",
    "                                param_attr='emb.w',\n",
    "                                is_sparse=False)\n",
    "        \n",
    "        # GRU\n",
    "        self._gru = fluid.dygraph.GRUUnit(\n",
    "            self.full_name(),\n",
    "            size=self.dec_units * 3,\n",
    "            param_attr=param_attr,\n",
    "            bias_attr=bias_attr,\n",
    "            activation=candidate_activation,\n",
    "            gate_activation=gate_activation,\n",
    "            origin_mode=origin_mode)\n",
    "        \n",
    "        # 如果维度不符合 D*3 不能传导，需要添加一层FC，保证维度是 D*3\n",
    "        self._fc4gru = fluid.dygraph.FC('fc_for_gru', self.dec_units*3)\n",
    "        \n",
    "        # FC (N,H)==>(N,V)\n",
    "        self._fc = fluid.dygraph.FC('fc', self.vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden, context_vector):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        # initial hidden is the last step of enc_output, hidden shape == (batch_size, hidden_size)\n",
    "        inputs = self._embedding(inputs)\n",
    "        # after concat shape is (batch_size, context_size + embedding_size)\n",
    "        inputs = fluid.layers.concat([context_vector, inputs], axis=-1)\n",
    "        inputs = self._fc4gru(inputs)\n",
    "        \n",
    "        hidden, reset, gate = self._gru(inputs, hidden)\n",
    "        \n",
    "        output = self._fc(hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "        \n",
    "#         res = []\n",
    "#         gates = []\n",
    "#         for i in range(inputs.shape[1]):\n",
    "# #             if self.is_reverse:\n",
    "# #                 i = inputs.shape[1] - 1 - i\n",
    "# #             input_ = inputs[:, i:i + 1, :]\n",
    "            \n",
    "#             # get Attention context_vector, shape = (batch_size, hidden_size)\n",
    "#             attention_input = fluid.layers.reshape(hidden, [-1, 1, hidden.shape[1]])\n",
    "#             context_vector, _ = self._attention(attention_input, enc_output)\n",
    "            \n",
    "#             # emb_vector shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "# #             input_ = fluid.layers.reshape(input_, [-1, input_.shape[2]])\n",
    "#             emb_vector = self._embedding(inputs)\n",
    "            \n",
    "#             # concat context_vector and embedding_vector\n",
    "#             # input_ shape == (batch_size, 1, context_size + embedding_size)\n",
    "#             input_ = fluid.layers.concat([emb_vector, context_vector], axis=-1)\n",
    "#             input_ = fluid.layers.reshape(input_, [-1, 1, input_.shape[1]])\n",
    "            \n",
    "#             # According to paddlepaddle API, input_ size need to be 3 * H(dec_units)\n",
    "#             input_ = self._fc4gru(input_)\n",
    "#             hidden, reset, gate = self._gru(input_, hidden)\n",
    "#             res_one_step = self._fc(hidden)\n",
    "#             res_one_step = fluid.layers.reshape(res_one_step, [-1, 1, res_one_step.shape[1]])\n",
    "#             res.append(res_one_step)\n",
    "            \n",
    "#             gate_ = fluid.layers.reshape(\n",
    "#                 gate, [-1, 1, gate.shape[1]], inplace=False)\n",
    "#             gates.append(gate_)\n",
    "#         if self.is_reverse:\n",
    "#             res = res[::-1]\n",
    "#             gates = gates[::-1]\n",
    "#         res = fluid.layers.concat(res, axis=1)\n",
    "#         gates = fluid.layers.concat(gates, axis=1)\n",
    "#         return res, gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.wv_loader as wv_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(fluid.dygraph.Layer):\n",
    "    def __init__(self, name_scope, params):\n",
    "        super(Seq2Seq, self).__init__(name_scope)\n",
    "        self.embedding_matrix = wv_loader.load_embedding_matrix()\n",
    "        self.params = params\n",
    "        self.encoder = Encoder(name_scope  = 'encoder',\n",
    "                               enc_units   = params[\"enc_units\"],\n",
    "                               batch_size  = params[\"batch_size\"],\n",
    "                               word_vector = self.embedding_matrix)\n",
    "\n",
    "        self.attention = BahdanauAttention(params[\"attn_units\"])\n",
    "\n",
    "        self.decoder = Decoder(params[\"vocab_size\"],\n",
    "                               params[\"embed_size\"],\n",
    "                               self.embedding_matrix,\n",
    "                               params[\"dec_units\"],\n",
    "                               params[\"batch_size\"])\n",
    "\n",
    "    def call_encoder(self, enc_inp):\n",
    "        enc_hidden = self.encoder.initialize_hidden_state()\n",
    "        enc_output, enc_hidden = self.encoder(enc_inp, enc_hidden)\n",
    "        return enc_output, enc_hidden\n",
    "\n",
    "    def call_decoder_onestep(self, dec_input, dec_hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(dec_hidden, enc_output)\n",
    "\n",
    "        pred, dec_hidden = self.decoder(dec_input,\n",
    "                                        None,\n",
    "                                        context_vector)\n",
    "        return pred, dec_hidden, context_vector, attention_weights\n",
    "\n",
    "    def call(self, dec_hidden, enc_output, dec_target):\n",
    "        predictions = []\n",
    "        attentions = []\n",
    "\n",
    "        context_vector, _ = self.attention(dec_hidden, enc_output)\n",
    "        dec_input = fluid.layers.reshape(dec_target[:, 0], [dec_target.shape[0],1,-1])\n",
    "\n",
    "        for t in range(1, dec_target.shape[1]):\n",
    "            pred, dec_hidden = self.decoder(dec_input,\n",
    "                                            dec_hidden,\n",
    "                                            context_vector)\n",
    "\n",
    "            context_vector, attn = self.attention(dec_hidden, enc_output)\n",
    "            # using teacher forcing\n",
    "            dec_input = fluid.layers.reshape(dec_target[:, t], [dec_target.shape[0],1,-1])\n",
    "\n",
    "            predictions.append(pred)\n",
    "            attentions.append(attn)\n",
    "\n",
    "            predictions = fluid.layers.concat(predictions, axis=1)\n",
    "            attentions = fluid.layers.concat(attentions, axis=1)\n",
    "\n",
    "        return predictions, attentions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试桩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34252, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils.wv_loader as wv_loader\n",
    "emb_matrix = wv_loader.load_embedding_matrix()\n",
    "emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([100, 64], [100, 10, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_hidden.shape, enc_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 1, 64]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fluid.layers.reshape(dec_hidden, shape=[dec_hidden.shape[0],1,-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape is (100, 10, 64)\n",
      "Encoder state shape is (100, 64)\n",
      "Decoder preds shape is (100, 10, 64)\n"
     ]
    }
   ],
   "source": [
    "use_emb = True\n",
    "with fluid.dygraph.guard():\n",
    "    # Encoder\n",
    "    enc_units = 64\n",
    "    batch_size = 100\n",
    "    T = 10\n",
    "    if use_emb:\n",
    "        encoder = Encoder('encoder', enc_units=enc_units, batch_size=batch_size, word_vector=emb_matrix)\n",
    "    else:\n",
    "        encoder = Encoder('encoder', enc_units=enc_units, batch_size=batch_size)\n",
    "    X = fluid.dygraph.base.to_variable(np.random.randint(emb_matrix.shape[0], size=(batch_size, T, 1)))\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    enc_outputs, enc_states = encoder(X, hidden)\n",
    "    print(\"Encoder output shape is {}\".format(enc_outputs.numpy().shape))\n",
    "    print(\"Encoder state shape is {}\".format(enc_states.numpy().shape))\n",
    "    attention = BahdanauAttention('attention', 10)\n",
    "    \n",
    "    # Decoder\n",
    "    dec_units = 64\n",
    "    batch_size = 100\n",
    "    if use_emb:\n",
    "        decoder = Decoder('decoder', dec_units=dec_units, batch_size=batch_size, word_vector=emb_matrix)\n",
    "    else:\n",
    "        decoder = Decoder('decoder', dec_units=dec_units, batch_size=batch_size)\n",
    "    X = fluid.dygraph.base.to_variable(np.random.randint(emb_matrix.shape[0], size=(batch_size, T, 1)))\n",
    "    # initial dec_hidden with encoder output hidden\n",
    "    dec_hidden = enc_states\n",
    "    preds = []\n",
    "    for t in range(T):\n",
    "        context_vector, _ = attention(fluid.layers.reshape(dec_hidden, shape=[dec_hidden.shape[0],1,dec_hidden.shape[1]]), enc_outputs)\n",
    "        x = X[:,t,:]\n",
    "        dec_hidden, _ = decoder(x, dec_hidden, context_vector)\n",
    "        preds.append(fluid.layers.reshape(dec_hidden, [batch_size, 1, dec_units]))\n",
    "    preds = fluid.layers.concat(preds, axis=1)\n",
    "    print(\"Decoder preds shape is {}\".format(preds.numpy().shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "wv_model = FastText.load('output/fasttext/fasttext.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器调用api\n",
    "optimizer = fluid.optimizer.AdamOptimizer(learning_rate=0.2, regularization=fluid.regularizer.L2Decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义loss函数\n",
    "def loss_function(real, pred):\n",
    "    # 判断logit为1和0的数量\n",
    "    real = fluid.layers.cast(real, dtype=np.int64)\n",
    "    mask = fluid.layers.logical_not(fluid.layers.equal(real, fluid.layers.ones_like(real)))\n",
    "    # 计算decoder的长度\n",
    "    dec_lens = fluid.layers.reduce_sum(fluid.layers.cast(mask, dtype=np.float32), dim=-1)\n",
    "    # 计算loss值\n",
    "    loss_ = fluid.layers.cross_entropy(input=pred, label=real)\n",
    "    # 转换mask的格式\n",
    "    mask = fluid.layers.cast(mask, dtype=loss_.dtype)\n",
    "    # 调整loss\n",
    "    loss_ *= mask\n",
    "    # 确认下是否有空的摘要别加入计算\n",
    "    loss_ = fluid.layers.reduce_sum(loss_, dim=-1) / real.shape[0]\n",
    "    return fluid.layers.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name tmp_8, dtype: VarType.FP64 shape: [1] \tlod: {}\n",
      "\tdim: 1\n",
      "\tlayout: NCHW\n",
      "\tdtype: double\n",
      "\tdata: [0.03153]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with fluid.dygraph.guard():\n",
    "    real = fluid.dygraph.base.to_variable(np.array([0,1,2]))\n",
    "    real = fluid.layers.reshape(real, [-1, 1])\n",
    "    pred = fluid.dygraph.base.to_variable(np.array([[.9, .05, .05], [.05, .89, .06], [.05, .01, .94]]))\n",
    "    loss = loss_function(real, pred)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2019-12-14 17:08:01,309 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/w3/yc8mtbd91vs80rp79zfgk8x00000gn/T/jieba.cache\n",
      "2019-12-14 17:08:01,311 : DEBUG : Loading model from cache /var/folders/w3/yc8mtbd91vs80rp79zfgk8x00000gn/T/jieba.cache\n",
      "Loading model cost 0.630 seconds.\n",
      "2019-12-14 17:08:01,940 : DEBUG : Loading model cost 0.630 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-12-14 17:08:01,941 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from utils.data_loader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train,X_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float')\n",
    "Y_train = Y_train.astype('float')\n",
    "X_test = X_test.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file((X_train,Y_train,X_test),'data/dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-14 17:03:44,051 : INFO : loading Word2Vec object from /Users/mac/Documents/baidu-ai-course/competition/BaiduAIProject-car/output/word2vec/word2vec.model\n",
      "2019-12-14 17:03:44,632 : INFO : loading wv recursively from /Users/mac/Documents/baidu-ai-course/competition/BaiduAIProject-car/output/word2vec/word2vec.model.wv.* with mmap=None\n",
      "2019-12-14 17:03:44,633 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-12-14 17:03:44,634 : INFO : loading vocabulary recursively from /Users/mac/Documents/baidu-ai-course/competition/BaiduAIProject-car/output/word2vec/word2vec.model.vocabulary.* with mmap=None\n",
      "2019-12-14 17:03:44,634 : INFO : loading trainables recursively from /Users/mac/Documents/baidu-ai-course/competition/BaiduAIProject-car/output/word2vec/word2vec.model.trainables.* with mmap=None\n",
      "2019-12-14 17:03:44,634 : INFO : setting ignored attribute cum_table to None\n",
      "2019-12-14 17:03:44,636 : INFO : loaded /Users/mac/Documents/baidu-ai-course/competition/BaiduAIProject-car/output/word2vec/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "from utils.wv_loader import *\n",
    "w2v_model = load_w2v_model(config.save_wv_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建Word2vec Vocab...\n"
     ]
    }
   ],
   "source": [
    "# Word2vec Embedding Matrix\n",
    "print('构建Word2vec Vocab...')\n",
    "w2v_matrix = w2v_model.wv.vectors\n",
    "save_file(w2v_matrix, config.embedding_matrix_path)\n",
    "\n",
    "# 保存id2word和word2id\n",
    "vocab = w2v_model.wv.vocab\n",
    "id2word = {i:x[0] for i,x in enumerate(vocab.items()) }\n",
    "word2id = {x:i for i,x in id2word.items()}\n",
    "save_file(id2word, config.id2word_path)\n",
    "save_file(word2id, config.word2id_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Vocab at 0x1a314434a8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = load_file(config.id2word_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = load_file(config.word2id_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34251"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'技师说：你这个有没有电脑检测故障代码。|车主说：有|技师说：发一下|车主说：发动机之前亮故障灯、显示是失火、有点缺缸、现在又没有故障、发动机多少有点抖动、检查先前的故障是报这个故障|车主说：稍等|车主说：显示图片太大传不了|技师说：[语音]|车主说：这个对发动机的抖动、失火、缺缸有直接联系吗？|技师说：[语音]|车主说：还有就是报（左右排气凸轮轴作动电磁铁）对正极短路、对地短路、对导线断路|技师说：[语音]|车主说：这几个电磁阀和问您的第一个故障有直接关系吧|技师说：[语音]|车主说：这个有办法检测它好坏吗？|技师说：[语音]|车主说：谢谢|技师说：不客气'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_raw_train = pd.read_csv(config.train_data_path)\n",
    "df_raw_train['Dialogue'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7f71a58ec5f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df['X'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df['X'][0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaikeba_lecture01",
   "language": "python",
   "name": "kaikeba_lecture01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
