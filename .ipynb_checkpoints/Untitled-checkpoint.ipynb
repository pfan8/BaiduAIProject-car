{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2019-12-29 01:42:30,042 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/w3/yc8mtbd91vs80rp79zfgk8x00000gn/T/jieba.cache\n",
      "2019-12-29 01:42:30,044 : DEBUG : Loading model from cache /var/folders/w3/yc8mtbd91vs80rp79zfgk8x00000gn/T/jieba.cache\n",
      "Loading model cost 0.647 seconds.\n",
      "2019-12-29 01:42:30,690 : DEBUG : Loading model cost 0.647 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-12-29 01:42:30,691 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import utils.config as config\n",
    "from utils.multi_proc_utils import parallelize\n",
    "from utils.wv_loader import get_vocab\n",
    "from utils.pickle_io import *\n",
    "import re\n",
    "import jieba\n",
    "jieba.load_userdict(config.user_dict)\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_pre(sentence):\n",
    "    # 要加空格，否则会连接在一起\n",
    "    sentence = re.sub('车主说', ' TOKEN1 ', sentence, flags=re.MULTILINE)\n",
    "    sentence = re.sub('技师说', ' TOKEN2 ', sentence, flags=re.MULTILINE)\n",
    "    sentence = re.sub('\\[图片\\]', ' TOKEN3 ', sentence, flags=re.MULTILINE)\n",
    "    sentence = re.sub('\\[语音\\]', ' TOKEN4 ', sentence, flags=re.MULTILINE)\n",
    "    sentence = re.sub('(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|#|\\-)*\\b', ' TOKEN5 ', sentence, flags=re.MULTILINE)\n",
    "    sentence = re.sub('\\|','',sentence, flags=re.MULTILINE)\n",
    "    # 训练词向量时，已加载词典到jieba，直接调用\n",
    "    words = jieba.cut(sentence)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def process_seq2seq(df):\n",
    "    '''\n",
    "    seq2seq批量处理方法\n",
    "    :param df: 数据集\n",
    "    :return:处理好的数据集\n",
    "    '''\n",
    "    # 批量预处理 训练集和测试集\n",
    "    for col_name in ['Question', 'Dialogue']:\n",
    "        df[col_name] = df[col_name].apply(seq2seq_pre)\n",
    "\n",
    "    if 'Report' in df.columns:\n",
    "        # 训练集 Report 预处理\n",
    "        df['Report'] = df['Report'].apply(seq2seq_pre)\n",
    "    return df\n",
    "\n",
    "def mark_proc(sentence, max_len, vocab, update=False):\n",
    "    '''\n",
    "    < start > < end > < pad > < unk >\n",
    "    '''\n",
    "    # 0.按空格统计切分出词\n",
    "    words = sentence.strip().split(' ')\n",
    "    # 1.过滤过多空格导致的空值''\n",
    "    words = [x for x in words if len(x)]\n",
    "    # 2. 截取规定长度的词数\n",
    "    words = words[:max_len]\n",
    "    if update:\n",
    "        sentence = words\n",
    "    else:\n",
    "        # 5. 填充< unk > ,判断是否在vocab中, 不在填充 < unk >\n",
    "        sentence = [word if word in vocab else '<UNK>' for word in words]\n",
    "        # 3. 填充< start > < end >\n",
    "        sentence = ['<START>'] + sentence + ['<STOP>']\n",
    "        # 4. 判断长度，填充　< pad >\n",
    "        sentence = sentence + ['<PAD>'] * (max_len - len(words))\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "def get_max_len(data):\n",
    "    \"\"\"\n",
    "    获得合适的最大长度值\n",
    "    :param data: 待统计的数据  train_df['Question']\n",
    "    :return: 最大长度值\n",
    "    \"\"\"\n",
    "    max_lens = data.apply(lambda x: x.count(' ')+1)\n",
    "    return int(np.mean(max_lens) + 2 * np.std(max_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config.train_data_path)\n",
    "test_df = pd.read_csv(config.test_data_path)\n",
    "train_df.dropna(subset=['Question','Dialogue','Report'], how='any', inplace=True)\n",
    "test_df.dropna(subset=['Question','Dialogue'], how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = parallelize(train_df, process_seq2seq)\n",
    "test_df = parallelize(test_df, process_seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建DataSet\n",
    "train_df['X'] = train_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x), axis=1)\n",
    "test_df['X'] = test_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaikeba_lecture01",
   "language": "python",
   "name": "kaikeba_lecture01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
