{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder And Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/tensorflow/nmt/blob/master/nmt/g3doc/img/attention_equation_0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 . GPU测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gpu_device_name = tf.test.gpu_device_name()\n",
    "# print(gpu_device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显存限制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，TensorFlow会映射该进程可见的所有GPU的几乎所有GPU内存（取决于CUDA_VISIBLE_DEVICES）。 这样做是为了通过减少内存碎片来更有效地使用设备上相对宝贵的GPU内存资源。 要将TensorFlow限制为一组特定的GPU，我们使用tf.config.experimental.set_visible_devices方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only use the first GPU\n",
    "#     try:\n",
    "#         tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "#     except RuntimeError as e:\n",
    "#         # Visible devices must be set before GPUs have been initialized\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在某些情况下，希望该过程仅分配可用内存的子集，或者仅增加该过程所需的内存使用量。 TensorFlow提供了两种控制方法。\n",
    "\n",
    "第一种选择是通过调用tf.config.experimental.set_memory_growth来打开内存增长，tf.config.experimental.set_memory_growth尝试仅分配运行时分配所需的GPU内存：它开始分配的内存很少，并且随着程序的运行和 需要更多的GPU内存，我们扩展了分配给TensorFlow进程的GPU内存区域。 请注意，我们不会释放内存，因为它可能导致内存碎片。 要打开特定GPU的内存增长，请在分配任何张量或执行任何操作之前使用以下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#     # Currently, memory growth needs to be the same across GPUs\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#             logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#             print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#     except RuntimeError as e:\n",
    "#         # Memory growth must be set before GPUs have been initialized\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启用此选项的另一种方法是将环境变量TF_FORCE_GPU_ALLOW_GROWTH设置为true。 此配置是特定于平台的。\n",
    "\n",
    "第二种方法是使用tf.config.experimental.set_virtual_device_configuration配置虚拟GPU设备，并对要在GPU上分配的总内存设置硬限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您想真正绑定TensorFlow进程可用的GPU内存量，这将很有用。 当GPU与其他应用程序（例如工作站GUI）共享GPU时，这是本地开发的常见做法。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今天我们来做NLP（自然语言处理）中Sequence2Sequence的任务。其中Sequence2Sequence任务在生活中最常见的应用场景就是机器翻译。除了机器翻译之外，现在很流行的对话机器人任务，摘要生成任务都是典型的Sequence2Sequence。Sequence2Sequence的难点在于模型需要干两件比较难的事情：\n",
    "\n",
    "+ 语义理解（NLU:Natural Language Understanding）：模型必须理解输入的句子。\n",
    "+ 句子生成(NLG:Natural Language Generation)：模型生成的句子需符合句法，不能是人类觉得不通顺的句子。\n",
    "\n",
    "想想看，让模型理解输入句子的语义已经很困难了，还得需要它返回一个符合人类造句句法的序列。不过还是那句话，没有什么是深度学习不能解决的，如果有，当我没说上句话。\n",
    "\n",
    "1. basic encoder-decoder ：将encode出来的编码全部丢给decode每个step\n",
    "![](https://upload-images.jianshu.io/upload_images/9168245-6e5404af1d88efa6.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/509/format/webp)\n",
    "\n",
    "2. encoder-decoder with feedback ：将encode出来的编码只喂给decode的初始step，在解码器端，需将每个step的输出，输入给下一个step。\n",
    "![](https://upload-images.jianshu.io/upload_images/9168245-e80ad756cad700df.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/514/format/webp)\n",
    "\n",
    "\n",
    "3. encoder-decoder with peek：1和2的组合，不仅将encode出来的编码全部丢给decode每个step，在解码器端，也将每个step的输出，输入给下一个step。\n",
    "![](https://upload-images.jianshu.io/upload_images/9168245-fd3175be92533464.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/544/format/webp)\n",
    "\n",
    "4.encoder-decoder with attention:将3模型的encode端做了一个小小的改进，加入了attention机制，简单来说，就是对encode端每个step的输入做了一个重要性打分。\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/9168245-e3773b933a5c359c.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/519/format/webp)\n",
    "\n",
    "本次实验采用的是basic encoder-decoder架构，下面开始实战部分。\n",
    "\n",
    "对对联实战\n",
    "数据加载\n",
    "数据样式如下图所示是一对对联。模型的输入时一句\"晚 风 摇 树 树 还 挺\"，需要模型生成\" 晨 露 润 花 花 更 红\"。这个数据集有个特点，就是输入输出是等长的，序列标注算法在这个数据集上也是适用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "# 要导入代码的路径 ,utils无法导入的同学,添加上自己code的路径 ,项目代码结构 code/utils ....\n",
    "sys.path.append('/home/roger/kaikeba/03_lecture/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.483 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.483 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.data_loader import build_dataset,load_dataset\n",
    "from utils.wv_loader import load_embedding_matrix,load_vocab\n",
    "from utils.config import *\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "import tensorflow as tf\n",
    "from model_layer import seq2seq_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_dataset(train_data_path,test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root='data/couplet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (root+\"/train/in.txt\",\"r\") as f:\n",
    "    data_in = f.read()\n",
    "with open (root+\"/train/out.txt\",\"r\") as f:\n",
    "    data_out = f.read()\n",
    "    \n",
    "train_X = data_in.split(\"\\n\")\n",
    "train_Y = data_out.split(\"\\n\")\n",
    "\n",
    "train_X = [data.split() for data in train_X]\n",
    "train_Y = [data.split() for data in train_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['万', '方', '乐', '奏', '有', '于', '阗'], ['绿', '柳', '堤', '新', '燕', '复', '来']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行上方代码将数据变成list，其格式如下：\n",
    "data_in_list[1:3] :\n",
    "\n",
    "[['愿', '景', '天', '成', '无', '墨', '迹'], ['丹', '枫', '江', '冷', '人', '初', '去']]\n",
    "\n",
    "data_out_list[1:3]:\n",
    "\n",
    "[['万', '方', '乐', '奏', '有', '于', '阗'], ['绿', '柳', '堤', '新', '燕', '复', '来']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# 获取所有的字\n",
    "words = list(itertools.chain.from_iterable(train_X))+list(itertools.chain.from_iterable(train_Y))\n",
    "# 去重\n",
    "words = set(words)\n",
    "# 构建vocab\n",
    "vocab = {word:index+1 for index ,word in enumerate(words)}\n",
    "# 添加unk标签\n",
    "vocab[\"unk\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上方代码构造一个字典，其格式如下所示，字典的作用就是将字变成计算机能处理的id。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{'罇': 1,\n",
    " '鳣': 2,\n",
    " '盘': 3,\n",
    " ...\n",
    " '弃': 168,\n",
    " '厌': 169,\n",
    " '楞': 170,\n",
    " '杋': 171,\n",
    "  ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 转换成索引\n",
    "train_X_ids = [[vocab.get(word,0) for word in sen] for sen in train_X]\n",
    "train_Y_ids = [[vocab.get(word,0) for word in sen] for sen in train_Y]\n",
    "# 填充长度\n",
    "train_X_ids = pad_sequences(train_X_ids,maxlen=100,padding='post')\n",
    "train_Y_ids = pad_sequences(train_Y_ids,maxlen=100,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 扩展维度\n",
    "train_Y_ids = train_Y_ids.reshape(*train_Y_ids.shape, 1)\n",
    "# train_label_input = train_label.reshape(*train_label.shape, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行上方代码将数据padding成等长(100维)，后续方便喂给模型。其中需要注意的是需要给train_label扩充一个维度，原因是由于keras的sparse_categorical_crossentropy loss需要输入的3维的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN  GRU  LSTM\n",
    "\n",
    "长文本 2000\n",
    "\n",
    "80% 90% \n",
    "\n",
    "RNN 40%  GRU 50%  LSTM 60%  梯度消失和爆炸  \n",
    "\n",
    "600 > step \n",
    "\n",
    "RNN 80%  GRU 80%  LSTM 80%  梯度消失和爆炸  \n",
    "\n",
    "TextRNN TextCNN \n",
    "\n",
    "TextRNN 80%  TextCNN 90%  解决:梯度消失和爆炸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网页分类 -- 很多文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 128)          1167744   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 256)          263168    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 100, 9123)         2344611   \n",
      "=================================================================\n",
      "Total params: 4,071,587\n",
      "Trainable params: 4,071,587\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import GRU, LSTM,Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "def seq2seq_model(input_length,output_sequence_length,vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size,output_dim = 128,input_length=input_length))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences = False)))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
    "    model.add(TimeDistributed(Dense(vocab_size, activation = 'softmax')))\n",
    "    model.compile(loss = sparse_categorical_crossentropy, \n",
    "                  optimizer = Adam(1e-3))\n",
    "    model.summary()\n",
    "    return model\n",
    "model = seq2seq_model(train_X_ids.shape[1],train_Y_ids.shape[1],len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型构建，keras可以很方便的帮助我们构建seq2seq模型，这里的encode 和decode采用的都是双向LSTM。其中RepeatVector(output_sequence_length) 这一步，就是执行将encode的编码输入给decode的每一个step的操作。从下图的模型可视化输出可以看到这个basic的seq2seq有39万多个参数需要学习."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://upload-images.jianshu.io/upload_images/9168245-406f70040286f6cc.png?imageMogr2/auto-orient/strip|imageView2/2/w/701/format/webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型构建好之后，就可以开始训练起来了。需要做的是将输入数据喂给模型，同时定义好batch_size和epoch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 616393 samples, validate on 154099 samples\n",
      "616393/616393 [==============================] - 1757s 3ms/sample - loss: 1.5081 - val_loss: 1.5093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc9adfbd5c0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X_ids,train_Y_ids, batch_size =64, epochs =10, validation_split = 0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire model to a HDF5 file\n",
    "model.save('data/epochs_10_batch_64_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the exact same model, including weights and optimizer.\n",
    "model = tf.keras.models.load_model('data/epochs_10_batch_64_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图是模型训练的过程，一个epoch大概需要近1小时，loss缓慢降低中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['散', '粜', '劭', '流', '齊', '散', '粜']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 飞流直下三千尺\n",
    "input_sen =\"飞流直下三千尺\"\n",
    "char2id = [vocab.get(i,0) for i in input_sen]\n",
    "input_data = pad_sequences([char2id],100)\n",
    "result = model.predict(input_data)[0][-len(input_sen):]\n",
    "result_label = [np.argmax(i) for i in result]\n",
    "dict_res = {i:j for j,i in vocab.items()}\n",
    "print([dict_res.get(i) for i in  result_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练10个epoch后，是时候考考模型的对对联实力了，运行上方代码，就可以看到模型的预测效果。\n",
    "“国破山河在”对出“人来日月长”，确实很工整。看来模型学习得不错，晋升为江湖第一对穿肠啦。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq在解决句子生成任务确实实力雄厚，仅仅只用了最basic的ecode和decode就能对出如此工整的句子（当然不是所有的句子都能对得这么好）。如果使用更强的模型训练对对联模型，实力应该可以考个古代状元。所以，大家有没有开始对深度学习处理NLP问题产生好奇，学习起来吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "\n",
    "https://kexue.fm/archives/6270\n",
    "\n",
    "https://blog.csdn.net/sinat_26917383/article/details/75050225"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaikeba_lecture01",
   "language": "python",
   "name": "kaikeba_lecture01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
